---
title: "Raman variability"
author: "Cristina Garcia-Timmermans, Dmitry Khalenow, Ruben Props & FM Kerckhof"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    number_sections: yes
vignette: >
  %\VignetteIndexEntry{Raman Variability}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(MicroRaman)
library(magrittr)
library(RColorBrewer)
library(dendextend)
```

# Goal

New checks to see Raman Variability

  1) Reproducibility: Ecoli 2092 grown in LB and NB, in biological triplicates, 28°C, 120rmp. After 24h, cells are measured using FCM and fixed with PFA. They are measured that day (day1, all samples). Check biological triplicates grown in LB and NB  
  
  2) Storage effect:
    Ecoli 2092 grown in LB and NB, in biological triplicates, 28°C, 120rmp. After 24h, cells are measured using FCM and fixed with PFA. They are measured that day (day1, all samples) and stored in PBS and measured again on day5(NB rep3, LBrep1) and day12(NB rep3, LBrep1)
      
  3) Dry effect
   Ecoli 2092 grown in LB, 28°C, 120rmp. After 24h, cells are measured using FCM and fixed with PFA. Some drops of 2µ are put in a silica slide. They dry in around 10 minutes. Cells are immediately measured (0h) and then at 3h and 6h
   
  4) Spin effect
  Another sample from 'Dry effect' (same culture) fixed in the same way but resuspended with PBS 6 extra times. To check 1spin-6spin, we will use 2092_0h_1spin


Strain | Replicates/Treatment | User | Growth conditions
-------|------------|------|------------------
E. coli LMG 2092 | 1,2,3 | CGT | NB, 28°C 120rmp
E. coli LMG 2092 | 1,2,3 | CGT | LB, 28°C 120rmp
E. coli LMG 2092 | 1 | CGT | LB, 28°C 120rmp

  
# Procedure

For each dataset, we will look at the spectra, and preprocess them. 
We then will cut the peak at $\sim 1000~\text{cm}^{-1}$ (suspicious). Next, we will make dendrograms and cut at a height that will allow us to see the plots well. Then we will plot the spectra for each cluster.
Finally, we will use random forests (RF) to select the most relevant peaks for classification. We will also use the algorithm VSURF. Here, we will use the code from `RandomForest_RamanVariability`.

## Check triplicates in LB and NB (reproducibility)

In this example we used *E. coli* LMG 2092 with incubation at 28°C and 120rpm 
for 24h (see original publication, Garcia-Timermans *et al.* (2018)).

### Data conversions

```{r BiologicalTriplicatesDC}
library(MicroRaman)
library(RColorBrewer)
# basefolder = "~/Software_dev/MicroRamanData/BiologicalReplicates_2medium/"
# filenames <- list.files(basefolder, pattern=".spc")
# spx.all <- lapply(paste0(basefolder,filenames),read.spc)
 
data("spx.all")
mdqs <- lapply(spx.all,hs2mq)

#### Rename spectra in R ####
# Normally, you can directly use the filenames object if you create your own
# dataset:
# labels <- filenames
# however, in case of the test-dataset we need to extract the filenames first
labels <-  sub(pattern = ".*/(.*.spc)","\\1",
               x =sapply(mdqs,function(x)x@metaData$name))
Medium <- unlist(lapply(strsplit(labels,split="_"),function(x) (x[2])))
Replicate <- unlist(lapply(strsplit(labels,split="_"),function(x) (x[3])))
ID.spc <- unlist(lapply(strsplit(labels,split="_"),function(x) (x[4])))
ID <- unlist(lapply(strsplit(ID.spc,split=".spc"),function(x) (x[1])))

# create a vector with a name for every cell, in this case metadata created by
# concatenating the medium that was used and which biological replicate 
# the cell belonged to (i.e. the name of the sample the cell belongs to)
cnvect <- paste(Medium,Replicate)

# assign this information to the MALDIquant object
mdqs.rn <- lapply(seq_along(cnvect),
                    function(x){
                      mass.spectrum <- mdqs[[x]]
                      metaData(mass.spectrum) <- list(name=cnvect[x])
                      return(mass.spectrum)
                    }
                   )
```

### TRIMMING

Select the biologically relevant part of the fingerprint: 
$600 - 1800~\text{cm}^{-1}$ (for dueterium $400 - 3200~\text{cm}^{-1}$)

```{r trimming, fig.width=7,fig.height=5}
mdqs.trim <- trim(mdqs.rn, range=c(600, 1800))
wavelengths.trim <-  mass(mdqs.trim[[1]]) 
# 332 masses (intervals are unequally distributed)

# This plot shows that the intervals between the different 
# wavenumbers are not equal
intervalplot(wavelengths.trim)

```

### BASELINE CORRECTION

First we look what the impact of setting the `iterations` argument to the `SNIP`
method (Statistics-sensitive Non-linear Iterative Peak-clipping algorithm) in `MALDIquant::estimateBaseline`. For more information, see: `?MALDIquant::estimateBaseline`

```{r blciter, fig.width=7,fig.height=5, fig.cap=c("Baseline correction for different iterations of the first sample","Baseline correction for the optimal number iterations plotted with the second sample")} 
iteration.options <- c(5,10,20,30,40,50,100)
iterationsplot(mdqs.trim[[1]],iteration.options)
#lapply(mdqs.trim,iterationsplot,iteration.options)
# optimal number of iterations?
number.of.iterations <- 10
i <- 2 #change this to look at other spectra
iterationsplot(mdqs.trim[[i]],number.of.iterations)
```

Next, we actually correct the spectra and inspect the result.

```{r blc, fig.width=7,fig.height=5, fig.cap=c("Comparison between original and baseline corrected spectrum for sample 2","Baseline corrected spectrum for sample 2","Averaged spectrum for all baseline corrected samples (with SNIP method)")} 
# correct all spectra
mass.spectra.baseline.corr <- removeBaseline(mdqs.trim, 
                                             method="SNIP",
                                             iterations=number.of.iterations)

# plot a spectrum to see the effect
baselinecorrplot(mass.spectra.baseline.corr,mdqs.trim,i=2,addorig=TRUE)
baselinecorrplot(mass.spectra.baseline.corr,mdqs.trim,i=2,addorig=FALSE)

#plot the average of the spectra to see how it looks before cutting
averagedSpectra <- averageMassSpectra(mass.spectra.baseline.corr)
ramplot(averagedSpectra, col="indianred",main="SNIP baseline corrected averaged spectrum")
```

Subsequently, an unexpected peak was removed (see citation) and the result was
inspected. 

```{r blccut, fig.width=7,fig.height=5, fig.cap=c("Averaged spectrum for all baseline corrected samples (with SNIP method) with unexpected peak removed")} 
## cut region 900-1100 cm-1 - unexpected peak
mass.spectra.cut <- wlcutter(mass.spectra.baseline.corr)

#Visualize what was cut
averagedSpectra_2 <- averageMassSpectra(mass.spectra.cut)
ramplot(averagedSpectra_2, main=NULL)

#I am happy with the new mass.spectra, so I substitute it for mass.spectra.baseline.corr
mass.spectra.baseline.corr<-mass.spectra.cut
```


### Normalisation

Here, we normalize to have the total area under the peaks eqaul to 1. Normalisation helps compare spectral datasets, regardless of acquisition parameters. An alternative
normalisation strategy that is a possibility is to use the peak maximum normalisation. 


```{r normalisation, fig.width=7,fig.height=5, fig.cap=c("TIC normalized Raman spectrum of sample 2")} 
mq.norm <- calibrateIntensity(mass.spectra.baseline.corr, 
                              method="TIC",range=c(600, 1800))
ramplot(mq.norm[[2]],main="TIC normalized Raman spectrum")
```

### Calculate similarities and perform clustering

Here, we first transform the baseline-corrected and normalized spectrum back to
a hyperSpec object. Three options were used to calculate the similarities:

1. Using the existing ecological dissimilarity metrics as available through the 
   package `vegan` in it's function `vegdist`. In this case, the Bray-Curtis 
   index was used, because of our previence experience that it was useful for 
   single-cell data as derived by means of flow cytometry. 
2. An alternative similarity metric is the spectral contract angle (SCA). The 
   SCA measures the angle between two vectors corresponding to closely related 
   spectra to measure whether they are the same or not (Wan *et al.* 2002). 
   
Next, we perform hierarchical clustering using Ward's D2 linkage.

```{r similarities, fig.width=8,fig.height=5, fig.cap=c("SCA-based hierarchical clustering of the cells","Heatmap showing the full dataset","SCA-based hierarchical clustering of the cells, with visualisation of clusters at a height of 0.75","SCA-based hierarchical clustering of the cells colored per medium and with a shape per replicate, the colors of the clusters indicate 8 selected clusters (at h=0.75)","Separate trees for the first four clusters","Separate trees for the next four clusters")} 
hs.norm3 <- mq2hs(mq.norm) #conversion to hyperSpec object

### OPTION 1: Bray-curtis ------------------------------------------------------
library(vegan)        
diss <- vegan::vegdist(hs.norm3[[]], method = 'bray') #cell-level dissimilarities


### OPTION 2: spectral contrast angle ------------------------------------------
# for reasons of computation time during rebuilding of vignettes, the
# command below is commented out. It is recommended to re-activate it if you
# would like to test it's performance
# similarity.2 <- SCA.diss(hs.norm3)
data("SCA.similarity")
similarity.2 <- SCA.similarity
### making a dendrogram based on the calculated similarity matrix --------------
dendrogram <- hclust(similarity.2, method="ward.D2")
plot(dendrogram)

### export tree ---------------------------------------------------------------- 
library(ape)
dendrogram_phylo <- as.phylo(dendrogram)
write.tree(dendrogram_phylo, file = "Dendrogram_LBvsNB_Raman.nwk", digits = 10)

# The heatmap shows clear structers in the dataset. 
heatmap(as.matrix(similarity.2))

#New name to export to iTOL

dg.itol <- SCAtoItol(hs.norm3,Factor1 = Medium,Factor2=Replicate)
write.tree(dg.itol,file="Dendrogram_LBvsNB.nwk",digits=10)
### at the given height calculate what cell is in what cluster -----------------
clusters <- cluscutplot(dendrogram,h=0.75)

# The tree at 0.75 has 8 clusters (k)
# max(clusters) #=> 8
k <- 8
cols <- colorspace::rainbow_hcl(k)
dend <- as.dendrogram(dendrogram)
dend <- dendextend::color_branches(dend, k = k)


# Set colors and shapes code
groupCodes<- c(rep("LBrep1", 45), rep("LBrep2", 45), rep("LBrep3", 44), rep("NBrep1", 45), rep("NBrep2", 44), rep("NBrep3", 45))
rownames(hs.norm3) <- make.unique(groupCodes)

colorCodes <- c(LBrep1="steelblue1", LBrep2="steelblue", LBrep3="steelblue4", NBrep1="indianred1", NBrep2="indianred", NBrep3="indianred4")
dendextend::labels_colors(dend) <- colorCodes[groupCodes][order.dendrogram(dend)]

leaves_col<-colorCodes[groupCodes][order.dendrogram(dend)]

shapeCodes <- c(LBrep1=8, LBrep2=17, LBrep3=13, NBrep1=8, NBrep2=17, NBrep3=13)
leaves_pch<-shapeCodes[groupCodes][order.dendrogram(dend)]

dend %>% dendextend::set("leaves_pch", leaves_pch) %>%  # node point type
  #set("leaves_cex", 0.7) %>%  # node point size
  dendextend::set("leaves_col", leaves_col) %>% #node point color
  #set("branches_col", leaves_col) %>%
  #par(mar = rep(0,4))
  #circlize_dendrogram(dend, labels_track_height = NA, dend_track_height = .4) 
plot(main = "Phenotypes and replicates", ylab="Height", leaflab="none",  type = "rectangle")
legend('topright',c("LB rep1", "LB rep2", "LB rep3","NB rep1", "NB rep2", "NB rep3") , pch= c(8,17,13),col=c("steelblue1", "steelblue", "steelblue4", "indianred1","indianred","indianred4"))

### optional: a circular representation ----------------------------------------       
# library(circlize)
# par(mar = rep(0,4))
# circlize_dendrogram(dend, dend_track_height = 0.8) 
# circlize_dendrogram(dend, labels_track_height = NA, dend_track_height = .4) 


### Plot separated trees -------------------------------------------------------
dendhc <- as.hclust(dend)
dendhc$labels <- make.unique(base::labels(dend),sep="_")
dend <- as.dendrogram(dendhc)
labels_dend <- base::labels(dend)
groups <- dendextend::cutree(dend, k=8, order_clusters_as_data = TRUE)
dends <- list()

for(i in 1:k) {
  labels_to_keep <- labels_dend[i != groups]
  dends[[i]] <- dendextend::prune(dend, labels_to_keep)
}

par(mfrow = c(2,2))

for(i in 1:k) {
  plot(dends[[i]],
       main = paste0("Tree for cluster number ", i))
}
```

#### Visualizing spectra of selected clusters

After allowing SCA-based cluster delineation in a semi-automated way, it can be
a good idea to inspect the spectra of those clusters.

```{r groupspectra, fig.width=8,fig.height=5, fig.cap=c("Mean spectrum for the first 4 clusters","Mean spectrum for the second 4 clusters","SCA-based hierarchical clustering of the cells colored per medium and with a shape per replicate, the colors of the clusters indicate 8 selected clusters (at h=0.75)")} 
### Spectra according to clusters ----------------------------------------------
# by default plot calls plotspc for a hyperSpec object
# this will plot maximally 25 spectra from each group (see hy.getOption("plot.spc.nmax"))
# it is possible to also show the average, which is what we did here
# 
hs.norm3$clusters<- as.factor(cutree(dendrogram, k=8))

c1 <- subset(hs.norm3,clusters==1)
c2 <- subset(hs.norm3,clusters==2)
c3 <- subset(hs.norm3,clusters==3)
c4 <- subset(hs.norm3,clusters==4)
c5 <- subset(hs.norm3,clusters==5)
c6 <- subset(hs.norm3,clusters==6)
c7 <- subset(hs.norm3,clusters==7)
c8 <- subset(hs.norm3,clusters==8)

par(mfrow = c(2,2))
plot(c1,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 1"))
box()
plot(c2,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 2"))
box()
plot(c3,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 3"))
box()

plot(c4,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 4"))
box()
plot(c5,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 5"))
box()
plot(c6,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 6"))
box()
plot(c7,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 7"))
box()
plot(c8,func=mean,
     title.args=list(y=list(ylab="Normalized intensity (AU)"),
                     x=list(xlab=expression("Wavenumber (cm"^-1*")")),
                     main="Average spectrum for cluster 8"))
box()
### Manual cluster validation --------------------------------------------------
correct_C1<- grep("LB",rownames(c1))
correct_C2<- grep("LB",rownames(c2))
correct_C3<- grep("LB",rownames(c3))
correct_C4<- grep("NB",rownames(c4))
correct_C5<- grep("NB",rownames(c5))
correct_C6<- grep("NB",rownames(c6))
correct_C7<- grep("NB",rownames(c7))
correct_C8<- grep("NB",rownames(c8))

correct_cluster<- length(correct_C1)+length(correct_C2)+length(correct_C3)+
                  length(correct_C4)+length(correct_C5)+length(correct_C6)+
                  length(correct_C7)+length(correct_C8)

accuracy_cluster <- correct_cluster/length(hs.norm3)
```

The obtained clustering accuracy based upon the SCA with hierarchical clustering
using Ward's D2 linkage was `r scales::percent(accuracy_cluster)`.

### Dimensionality reduction

```{r ordination, include=FALSE, eval=FALSE} 

#### PCA ####
pca <- prcomp(hs.norm$.)
plot(pca)
summary(pca)

library('factoextra')
labels <- groupCodes

# PCA 
res.PCA <- prcomp(hs.norm$.) 
p <- fviz_pca_ind(res.PCA,label='none', geom ="point", habillage = labels,pointsize = 2)# addEllipses=TRUE, ellipse.level=0.95)
p +labs(title = "PCA" ) + theme_minimal()
p + scale_color_manual(values=c("steelblue1", "steelblue", "steelblue4", "indianred","indianred1","indianred4"))+
                       scale_shape_manual(values=c(8,17,13,8,17,13))

#kmeans clusters
library(ggfortify)
library(ggplot2)
library(RColorBrewer)
autoplot(pca, label=FALSE)
autoplot(pca,label = TRUE, label.size = 4,loadings = FALSE, loadings.label = FALSE)

# plotting the kmeans clusters on top of the data depending on the number of PCA's
library(cluster)
# for the first two
autoplot(kmeans(pca$x[,1:2], 6), data =pca$x[,1:2] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# 70%
autoplot(kmeans(pca$x[,1:20], 6), data =pca$x[,1:20] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# 80%
autoplot(kmeans(pca$x[,1:37], 6), data =pca$x[,1:37] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# 85%
autoplot(kmeans(pca$x[,1:49], 6), data =pca$x[,1:49] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# 90%
autoplot(kmeans(pca$x[,1:65], 6), data =pca$x[,1:65] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# 95%
autoplot(kmeans(pca$x[,1:88], 6), data =pca$x[,1:88] ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")
# for all
autoplot(kmeans(pca$x, 6), data =pca$x ,label = TRUE, label.size = 3,loadings = FALSE, loadings.label = FALSE, frame=TRUE, frame.type= "norm")


# how to get the percentage per principal component out of the prcomp object
PoV <- (pca$sdev)^2 / sum(pca$sdev^2)
cumPoV <- cumsum(pca$sdev^2 / sum(pca$sdev^2))
Var <- rbind(PoV, cumPoV)


#### Cluster validation ####
library(clValid)
library(kohonen)
cl.valid <- clValid(pca$x, 2:8, clMethods=c("hierarchical", "kmeans", "diana", "fanny", "som", "model", "sota", "pam", "clara"),validation = c("internal", "stability"))

hsnorm_stats <- cluster.stats(dist(hsnorm.df),  hs.norm$clusters)


```


# References

1. García-Timermans, C., Rubbens, P., Kerckhof, F. M., Buysschaert, B., Khalenkow, D., Waegeman, W., Skirtagh, A.G. & Boon, N. (2018). Label-free Raman characterization of bacteria calls for standardized procedures. *Journal of microbiological methods*, 151, 69-75.
2. Wan, K. X., Vidavsky, I., & Gross, M. L. (2002). Comparing similar spectra: from similarity index to spectral contrast angle. *Journal of the American Society for Mass Spectrometry*, 13(1), 85-88.
3. Gibb, S. and Strimmer, K. (2012). MALDIquant: a versatile R package for the analysis of mass spectrometry data. *Bioinformatics* 28, 2270-2271.
4. Claudia Beleites and Valter Sergo: `hyperSpec: a package to handle hyperspectral data sets in R', R   package version 0.99-20180627. http://hyperspec.r-forge.r-project.org
5. Letunic, I., & Bork, P. (2016). Interactive tree of life (iTOL) v3: an online tool for the display and annotation of phylogenetic and other trees. *Nucleic acids research*, 44(W1), W242-W245.

